{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Testing Python Code with Pytest"
      ],
      "metadata": {
        "id": "mjXut5IzHXJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing is a critical part of software development that helps ensure the correctness and reliability of your code. **Pytest** is a popular testing framework for Python that makes it easy to write and run tests for your Python code. In this tutorial, we'll cover the basics of using Pytest to test your Python code effectively."
      ],
      "metadata": {
        "id": "k1UdfIsjH9CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "Before you begin, make sure you have the following installed:\n",
        "\n",
        "Python: You'll need Python installed on your system. You can download it from python.org or use a package manager like `conda` or `brew` to install it.\n",
        "\n",
        "Pytest: Install Pytest using `pip`:"
      ],
      "metadata": {
        "id": "1uxtzpHFIjFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGo5n75dH8cz",
        "outputId": "74b7bea8-4417-4522-e8eb-753ba91ec6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest"
      ],
      "metadata": {
        "id": "EwrilVLjNBw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a Simple Test"
      ],
      "metadata": {
        "id": "UAtgzHXlIMQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by creating a simple Python module and then writing a test for it using Pytest."
      ],
      "metadata": {
        "id": "cZhb202-IPWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Create a Python Module\n",
        "Create a Python module (e.g., my_module.py) with a simple function to test. Here's an example module:"
      ],
      "metadata": {
        "id": "kn8RuAz_J5XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my_module.py\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b"
      ],
      "metadata": {
        "id": "Rzja8bhLIPBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Write a Test\n",
        "\n",
        "Create a test file (e.g., test_my_module.py) with test functions. Pytest uses a naming convention where test files should start with test_ and test functions should also start with test_. Here's a simple test for the add function:"
      ],
      "metadata": {
        "id": "fhYZXdDxKDAI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSHrxyE8GByK"
      },
      "outputs": [],
      "source": [
        "# test_my_module.py\n",
        "# from my_module import add\n",
        "\n",
        "def test_add():\n",
        "    assert add(2, 3) == 5\n",
        "    assert add(-1, 1) == 0\n",
        "    assert add(0, 0) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Run the Tests\n",
        "Now, you can run your tests using Pytest. Open your terminal and navigate to the directory containing your test files. Then, run:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "pytest\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "AFBdxZwgKppn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytest will automatically discover and run all the test functions in files that match the naming convention. You should see an output indicating that the tests passed:"
      ],
      "metadata": {
        "id": "opZgqaS2LDEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "=========================== test session starts ============================\n",
        "platform linux -- Python 3.8.10, pytest-6.2.4, pluggy-0.13.1\n",
        "rootdir: /path/to/your/project\n",
        "collected 1 item\n",
        "\n",
        "test_my_module.py .                                                  [100%]\n",
        "\n",
        "============================ 1 passed in 0.12s =============================\n"
      ],
      "metadata": {
        "id": "bJyDVsblKsoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You've successfully written and run a basic test using Pytest."
      ],
      "metadata": {
        "id": "V2nc9qqvLPr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Specific Tests\n",
        "You can run specific tests by specifying the test file and test function. For example:"
      ],
      "metadata": {
        "id": "R-Y_lvN4LdKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pytest test_my_module.py::test_add"
      ],
      "metadata": {
        "id": "Yyjjw1SiQRYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command runs only the `test_add function` in the `test_my_module.py` file."
      ],
      "metadata": {
        "id": "RtisJcRESbIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assertions and Test Outcomes\n",
        "In the test functions, we used assert statements to check if the actual results match the expected results. If an assert statement fails, Pytest will report a test failure.\n",
        "\n",
        "Pytest supports various assertion methods, such as `assert`, `assertEqual`, `assertTrue`, `assertFalse`, and more. You can choose the one that fits your testing needs."
      ],
      "metadata": {
        "id": "niLt7T-ISfCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Organizing Tests\n",
        "As your codebase grows, you may want to organize your tests into test suites or test classes. Pytest allows you to do this by creating classes for test cases or grouping tests using test modules and directories. For example:\n",
        "\n"
      ],
      "metadata": {
        "id": "MI05NO2jLtmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_math.py\n",
        "\n",
        "class TestAddition:\n",
        "    def test_add_positive_numbers(self):\n",
        "        assert add(2, 3) == 5\n",
        "\n",
        "    def test_add_negative_numbers(self):\n",
        "        assert add(-1, 1) == 0\n",
        "\n",
        "    def test_add_zeros(self):\n",
        "        assert add(0, 0) == 0"
      ],
      "metadata": {
        "id": "drvk8lcJL6P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we've organized tests related to the add function into a test class."
      ],
      "metadata": {
        "id": "eHm6idsmL5WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Fixtures when Pytesting\n",
        "Fixtures in Pytest are functions marked with the `@pytest.fixture` decorator. They provide a way to set up and clean up resources for your tests. Fixtures can be particularly useful for creating reusable setup code or managing resources like databases, files, or external services."
      ],
      "metadata": {
        "id": "BBpkqTt8Disk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Import Pytest and Define a Fixture\n",
        "You'll first need to import Pytest and define a fixture. Here are some example fixtures, one that returns a simple resource, and another that performs the same behavior, but with setup/teardown functionality:"
      ],
      "metadata": {
        "id": "33DzWjJCDupc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "\n",
        "# Define a fixture function\n",
        "@pytest.fixture()\n",
        "def some_resource():\n",
        "  # Setup: Code to create or prepare a resource\n",
        "  resource = \"This is a resource\"\n",
        "\n",
        "  return resource\n",
        "\n",
        "\n",
        "# Define a fixture function\n",
        "@pytest.fixture\n",
        "def some_resource_2():\n",
        "    # Setup: Code to create or prepare a resource\n",
        "    resource = \"This is a resource\"\n",
        "\n",
        "    yield resource  # The fixture provides this resource to the tests\n",
        "\n",
        "    # Teardown: Code to clean up or release the resource (optional)\n",
        "    # In this case, you might not need teardown for a simple resource\n",
        "    print(\"Tearing down the resource\")"
      ],
      "metadata": {
        "id": "vXxXvvj_DuDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Use the Fixture in Your Tests\n",
        "You can use the fixture in your test functions by including it as a parameter. Pytest will automatically inject the resource provided by the fixture:"
      ],
      "metadata": {
        "id": "dvpg8BA-EDk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a test that uses the fixture\n",
        "def test_with_fixture(some_resource):\n",
        "    assert some_resource == \"This is a resource\""
      ],
      "metadata": {
        "id": "XMadacrdEHm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `test_with_fixture` function, `some_resource` is automatically populated with the value provided by the `some_resource` fixture."
      ],
      "metadata": {
        "id": "uNmcfaouEK0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Run the Tests with Fixtures\n",
        "To run the tests that use fixtures in your Jupyter Notebook, simply execute the test functions as you normally would by running `pytest`.\n",
        "\n",
        "Pytest will handle setting up and tearing down the fixture before and after the test function runs."
      ],
      "metadata": {
        "id": "YqSMBOCgEW2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameterizing Tests with Pytest\n",
        "Parameterization is a powerful feature in Pytest that allows you to run a single test function with multiple input data sets. This is particularly useful when you have similar test cases that differ only in their inputs or expected outcomes. Parameterized tests can help you keep your test code clean and concise."
      ],
      "metadata": {
        "id": "09EWGV4SHeNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Pytest and a Parameterized Test\n",
        "Before using parameterization, ensure you have Pytest imported.\n",
        "\n",
        "To create a parameterized test, you can use the `@pytest.mark.parametrize` decorator. This decorator allows you to specify multiple sets of input data and expected outcomes for a test function. Here's an example:"
      ],
      "metadata": {
        "id": "z32wDocbHm2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "# Define a test function\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "# Parameterize the test function\n",
        "@pytest.mark.parametrize(\"input_a, input_b, expected\", [(2, 3, 5), (-1, 1, 0), (0, 0, 0)])\n",
        "def test_add(input_a, input_b, expected):\n",
        "    result = add(input_a, input_b)\n",
        "    assert result == expected"
      ],
      "metadata": {
        "id": "7tyXnRwxH4jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the `test_add` function is parameterized with three sets of input data and expected outcomes. Pytest will run the `test_add` function three times, once for each set of parameters."
      ],
      "metadata": {
        "id": "yqMsMRS3H7OC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Run the Parameterized Test\n",
        "You can run the parameterized test just like any other test function.\n",
        "\n",
        "Pytest will execute the `test_add` function three times, once for each set of parameters, and report the results individually for each set of inputs.\n",
        "\n",
        "Parameterizing Multiple Test Functions\n",
        "You can parameterize multiple test functions in the same test module or notebook. Just use the `@pytest.mark.parametrize` decorator for each test function that requires parameterization:"
      ],
      "metadata": {
        "id": "DiCCE-rmIBmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "# Define test functions\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def subtract(a, b):\n",
        "    return a - b\n",
        "\n",
        "# Parameterize multiple test functions\n",
        "@pytest.mark.parametrize(\"input_a, input_b, expected\", [(2, 3, 5), (-1, 1, 0), (0, 0, 0)])\n",
        "def test_add(input_a, input_b, expected):\n",
        "    result = add(input_a, input_b)\n",
        "    assert result == expected\n",
        "\n",
        "@pytest.mark.parametrize(\"input_a, input_b, expected\", [(5, 3, 2), (1, 2, -1), (0, 0, 0)])\n",
        "def test_subtract(input_a, input_b, expected):\n",
        "    result = subtract(input_a, input_b)\n",
        "    assert result == expected"
      ],
      "metadata": {
        "id": "26AsS8pxIAo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we parameterized both `test_add` and `test_subtract` functions with their respective input data sets.\n",
        "\n",
        "Benefits of Parameterized Tests\n",
        "Parameterized tests offer several advantages:\n",
        "\n",
        "1. **Code Reusability**: Reduce code duplication by writing a single test function that handles multiple test cases.\n",
        "\n",
        "2. **Clarity**: Clearly define and document the input data and expected outcomes for each test case in one place.\n",
        "\n",
        "3. **Maintainability**: Easily add new test cases without creating additional test functions.\n",
        "\n",
        "4. **Conciseness**: Simplify your test suite by eliminating redundant test functions.\n",
        "\n",
        "By following these steps, you can leverage parameterization in Pytest to streamline your testing process and ensure thorough coverage of your code with minimal effort."
      ],
      "metadata": {
        "id": "nh-1BhwyIXJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Subtests in Pytest\n",
        "\n",
        "Subtests are a powerful feature in Pytest that allow you to run multiple test cases within a single test function, ensuring that all test cases are executed, even if some of them fail. This is particularly useful in scenarios where you want to validate different inputs or edge cases comprehensively."
      ],
      "metadata": {
        "id": "eAzIkg-DIqUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use Subtests\n",
        "You should consider using subtests in the following scenarios:\n",
        "\n",
        "1. **Testing Multiple Input Cases**: When you want to test a function with various input values and ensure all test cases are executed, regardless of whether some fail."
      ],
      "metadata": {
        "id": "WnxuSMPoI63l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "def divide(a, b):\n",
        "    return a / b\n",
        "\n",
        "@pytest.mark.parametrize(\"a, b, expected\", [(4, 2, 2), (8, 4, 2), (10, 5, 2)])\n",
        "def test_divide(a, b, expected):\n",
        "    with pytest.subtests():\n",
        "        result = divide(a, b)\n",
        "        assert result == expected"
      ],
      "metadata": {
        "id": "4wmqcx_1IqAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, subtests ensure that all test cases within the `test_divide` function are executed, even if one of them fails.\n",
        "\n",
        "2. **Testing Edge Cases**: When you need to test edge cases to make sure all scenarios are covered without duplicating test functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "cckGUDHXJYkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "def is_prime(n):\n",
        "    if n <= 1:\n",
        "        return False\n",
        "    if n <= 3:\n",
        "        return True\n",
        "    if n % 2 == 0 or n % 3 == 0:\n",
        "        return False\n",
        "    i = 5\n",
        "    while i * i <= n:\n",
        "        if n % i == 0 or n % (i + 2) == 0:\n",
        "            return False\n",
        "        i += 6\n",
        "    return True\n",
        "\n",
        "@pytest.mark.parametrize(\"n, expected\", [(0, False), (1, False), (2, True), (3, True), (4, False)])\n",
        "def test_is_prime(n, expected):\n",
        "    with pytest.subtests():\n",
        "        result = is_prime(n)\n",
        "        assert result == expected"
      ],
      "metadata": {
        "id": "RnCTz2dTJiXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, subtests ensure that all test cases for the `is_prime` function are run, even if some of them result in failures.\n",
        "\n",
        "3. **Data Validation**: When validating a large dataset and you want to report all data validation issues in a single test.\n"
      ],
      "metadata": {
        "id": "p2bl5iXZJdZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "def validate_data(data):\n",
        "    for entry in data:\n",
        "        with pytest.subtests():\n",
        "            assert entry['value'] >= 0, \"Value must be non-negative\"\n",
        "            assert len(entry['name']) <= 50, \"Name should be less than 50 characters\"\n",
        "            # Add more validation checks as needed\n",
        "\n",
        "def test_validate_data():\n",
        "    data = [\n",
        "        {\"name\": \"John Doe\", \"value\": 42},\n",
        "        {\"name\": \"Alice\", \"value\": -10},\n",
        "        {\"name\": \"Bob\", \"value\": 100},\n",
        "    ]\n",
        "    validate_data(data)"
      ],
      "metadata": {
        "id": "1LBvoQmzJxtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subtests help ensure that the `validate_data` function checks each data entry and reports all validation issues.\n",
        "\n",
        "4. **Complex Scenarios**: In complex test scenarios where multiple conditions must be met, subtests provide clarity and organization.\n",
        "\n",
        "Using subtests in these scenarios helps maintain test clarity, allows for better organization, and ensures that all relevant test cases are executed even if some of them fail. It can be especially valuable when debugging test failures by providing a clear breakdown of which test cases succeeded and which ones failed within a single test function."
      ],
      "metadata": {
        "id": "_kBAo5wDJeQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "Pytest is a powerful and flexible testing framework that simplifies the process of writing and running tests for your Python code. In this tutorial, you've learned how to write basic tests, run them, and organize them effectively. As you continue to develop your Python projects, writing comprehensive tests with Pytest will help you ensure the reliability and correctness of your code."
      ],
      "metadata": {
        "id": "wpxtYG7fMBmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### Additional Best Practices\n",
        "\n",
        "When using Pytest in a Jupyter Notebook or any Python project, it's essential to follow best practices to ensure your tests are effective and maintainable:\n",
        "\n",
        "\n",
        "1.   Use Descriptive Test Names: Give your test functions and fixtures clear and descriptive names. Use underscores to separate words in function names for better readability. This makes it easier to understand the purpose of each test.\n",
        "2.   Organize Your Tests: Group related tests together using Markdown headings or by structuring your test files and functions logically.\n",
        "3. Keep Tests Isolated: Each test should be independent and not rely on the state or side effects of other tests. Use fixtures to set up and tear down resources as needed for each test.\n",
        "4. Use Fixtures Wisely: Use fixtures for setup and teardown, but avoid overusing them. If a resource doesn't require setup or cleanup, there's no need for a fixture.\n",
        "5. Use Assertions: Utilize Pytest's powerful assertion methods (`assert`, `assertEqual`, `assertTrue`, etc.) to check the correctness of your code.\n",
        "6. Run Tests Frequently: Run your tests regularly during development to catch issues early. You can automate this process with continuous integration (CI) tools like GitHub Actions or Travis CI.\n",
        "7. Parametrize Tests: When testing similar scenarios with different input values, consider using Pytest's parametrize feature to avoid duplicating test code.\n",
        "8. Test Edge Cases: Ensure that your tests cover edge cases and corner cases. Test for scenarios where inputs are at their minimum or maximum values or where unexpected input might occur.\n",
        "9. Test Failure Cases: Don't just test when things go right; test when things go wrong. Ensure your code handles exceptions and errors gracefully.\n",
        "10. Documentation: Provide clear documentation for your tests, especially when they involve complex scenarios or workarounds.\n",
        "\n"
      ],
      "metadata": {
        "id": "wrXaSJXyFC6b"
      }
    }
  ]
}